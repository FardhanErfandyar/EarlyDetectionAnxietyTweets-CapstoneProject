{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNm9kkuGKjaK8cqT42ja9Wa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FardhanErfandyar/EarlyDetectionAnxietyTweets-CapstoneProject/blob/main/Capstone_Project_Fardhan_Erfandyar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2be7864b"
      },
      "source": [
        "# Import Library\n",
        "%pip install langchain_community\n",
        "%pip install replicate\n",
        "import os\n",
        "import pandas as pd\n",
        "from langchain_community.llms import Replicate\n",
        "from getpass import getpass\n",
        "import logging\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 1. Setup Environment and API Token\n",
        "# ==============================================================================\n",
        "\n",
        "# API Token from Colab Secrets\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    REPLICATE_API_TOKEN = userdata.get('REPLICATE_API_TOKEN')\n",
        "    if REPLICATE_API_TOKEN is None:\n",
        "        raise KeyError\n",
        "    os.environ[\"REPLICATE_API_TOKEN\"] = REPLICATE_API_TOKEN\n",
        "    logging.info(\"Token Replicate berhasil dimuat dari Colab Secrets.\")\n",
        "except (ImportError, KeyError):\n",
        "    logging.error(\"Token Replicate tidak ditemukan di Colab Secrets. Harap ikuti panduan untuk menambahkannya.\")\n",
        "\n",
        "    try:\n",
        "        REPLICATE_API_TOKEN = getpass(\"Harap masukkan Replicate API Token Anda: \")\n",
        "        os.environ[\"REPLICATE_API_TOKEN\"] = REPLICATE_API_TOKEN\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Gagal mendapatkan REPLICATE_API_TOKEN: {e}\")"
      ],
      "metadata": {
        "id": "zpCtpsEP_HLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 2. Inisiate Model Openai O4 Mini\n",
        "# ==============================================================================\n",
        "model_id = \"openai/o4-mini\"\n",
        "\n",
        "# Tuning Paramenter MOdel\n",
        "llm = Replicate(\n",
        "    model=model_id,\n",
        "    model_kwargs={\n",
        "        \"temperature\": 0.5,\n",
        "        \"max_new_tokens\": 150,\n",
        "        \"top_k\": 50,\n",
        "        \"top_p\": 0.95,\n",
        "        \"repetition_penalty\": 1.1\n",
        "    }\n",
        ")\n",
        "logging.info(f\"Model {model_id} berhasil diinisialisasi.\")"
      ],
      "metadata": {
        "id": "O4ulwB0-_KX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 3. Preparing Data\n",
        "# ==============================================================================\n",
        "def prepare_data(train_path, test_path):\n",
        "    \"\"\"Memuat, mengelompokkan, dan menyiapkan data latih & uji.\"\"\"\n",
        "    try:\n",
        "        df_train = pd.read_csv(train_path)\n",
        "        df_test = pd.read_csv(test_path)\n",
        "\n",
        "        # Cleaning\n",
        "        df_train.columns = df_train.columns.str.strip()\n",
        "        df_test.columns = df_test.columns.str.strip()\n",
        "\n",
        "        logging.info(\"File training dan testing berhasil dimuat.\")\n",
        "    except FileNotFoundError as e:\n",
        "        logging.error(f\"Error: {e}. Pastikan kedua file CSV ada di direktori.\")\n",
        "        return None, None\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error saat memuat data: {e}\")\n",
        "        return None, None\n",
        "\n",
        "    # Group Training Data\n",
        "    if 'label' not in df_train.columns:\n",
        "        logging.error(\"Kolom 'label' tidak ditemukan di file training.\")\n",
        "        return None, None\n",
        "\n",
        "    train_grouped = df_train.groupby('username').agg({\n",
        "        'full_text': lambda tweets: list(tweets),\n",
        "        'label': 'first'\n",
        "    }).reset_index()\n",
        "\n",
        "    # Group Testing Data\n",
        "    test_grouped = df_test.groupby('username')['full_text'].apply(list).reset_index()\n",
        "\n",
        "    logging.info(f\"{len(train_grouped)} pengguna unik di data latih.\")\n",
        "    logging.info(f\"{len(test_grouped)} pengguna unik di data uji.\")\n",
        "\n",
        "    return train_grouped, test_grouped"
      ],
      "metadata": {
        "id": "X6zFXtKS_RPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 4. Create Shot Prompt\n",
        "# ==============================================================================\n",
        "def create_few_shot_prompt(train_df, user_to_classify_tweets):\n",
        "    \"\"\"Membuat prompt few-shot yang dinamis dari data latih.\"\"\"\n",
        "\n",
        "    anxious_examples = train_df[train_df['label'] == 1].head(2)\n",
        "    not_anxious_examples = train_df[train_df['label'] == 0].head(2)\n",
        "    examples = pd.concat([anxious_examples, not_anxious_examples])\n",
        "\n",
        "    example_text = \"\"\n",
        "    for _, row in examples.iterrows():\n",
        "        label_text = \"Anxious\" if row['label'] == 1 else \"Not Anxious\"\n",
        "        tweets_str = \"\\n\".join([f\"- {t}\" for t in row['full_text']])\n",
        "        example_text += f\"\"\"---\n",
        "**Example (Known Classification):**\n",
        "**User's Tweets:**\n",
        "{tweets_str}\n",
        "**Analysis:**\n",
        "- **Reasoning:** This user's tweets show a pattern of {('worry and self-doubt.' if label_text == 'Anxious' else 'general daily life topics.')}\n",
        "- **Classification:** {label_text}\n",
        "\"\"\"\n",
        "\n",
        "    # Format Tweets\n",
        "    user_tweets_str = \"\\n\".join([f\"- {t}\" for t in user_to_classify_tweets])\n",
        "\n",
        "    # Final Prompt\n",
        "    final_prompt = f\"\"\"You are a highly skilled psychological analyst specializing in detecting signs of anxiety from social media text.\n",
        "Your task is to analyze a collection of recent tweets from a single user and determine if their overall posting pattern indicates potential anxiety.\n",
        "\n",
        "**Definitions:**\n",
        "- **Anxious:** The user's tweets, when viewed together, show a recurring pattern of worry, fear, panic, overthinking, social withdrawal, or direct mentions of an anxiety diagnosis. The sentiment is often negative or tense.\n",
        "- **Not Anxious:** The user's tweets cover a variety of general topics without a recurring pattern of the anxious traits mentioned above.\n",
        "\n",
        "**Instructions:**\n",
        "1. Carefully read all the provided tweets from the user.\n",
        "2. Analyze the overall pattern, tone, and topics. Do not judge based on a single tweet.\n",
        "3. Provide your reasoning in one short sentence.\n",
        "4. Provide your final classification in the specified format.\n",
        "\n",
        "**LEARNING EXAMPLES:**\n",
        "{example_text}\n",
        "---\n",
        "**USER TO CLASSIFY:**\n",
        "**User's Tweets:**\n",
        "{user_tweets_str}\n",
        "\n",
        "**YOUR ANALYSIS:**\n",
        "- **Reasoning:**\n",
        "- **Classification:**\"\"\"\n",
        "\n",
        "    return final_prompt\n"
      ],
      "metadata": {
        "id": "PJxM1R61_ZBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jz_F6c1roHrI"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# 5. Main Function\n",
        "# ==============================================================================\n",
        "def run_project():\n",
        "    train_df, test_df = prepare_data(\n",
        "        '/content/final_training_dataset.csv',\n",
        "        '/content/final_testing_dataset.csv'\n",
        "    )\n",
        "\n",
        "    if train_df is None or test_df is None:\n",
        "        logging.error(\"Data preparation failed. Exiting.\")\n",
        "        return\n",
        "\n",
        "    results = []\n",
        "    # Progress Bar\n",
        "    for index, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Classifying Accounts\"):\n",
        "        username = row['username']\n",
        "        tweets = row['full_text']\n",
        "\n",
        "        print(f\"\\n---> Menganalisis akun: @{username}...\")\n",
        "\n",
        "        # Prompt\n",
        "        prompt = create_few_shot_prompt(train_df, tweets)\n",
        "\n",
        "        # Call LLM Openai 04 Mini\n",
        "        response = llm.invoke(prompt)\n",
        "\n",
        "        # Output Model\n",
        "        reasoning = \"N/A\"\n",
        "        classification = \"Unknown\"\n",
        "\n",
        "        lines = response.strip().split('\\n')\n",
        "        for line in lines:\n",
        "            if \"Reasoning:\" in line:\n",
        "                reasoning = line.replace(\"Reasoning:\", \"\").strip()\n",
        "            if \"Classification:\" in line:\n",
        "                classification = line.replace(\"Classification:\", \"\").strip()\n",
        "\n",
        "        results.append({\n",
        "            'username': username,\n",
        "            'predicted_condition': classification,\n",
        "            'reasoning': reasoning,\n",
        "            'tweet_count': len(tweets)\n",
        "        })\n",
        "        print(f\"  -> Hasil untuk @{username}: {classification} (Alasan: {reasoning})\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run_project()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LIXTY0jl-3xf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}